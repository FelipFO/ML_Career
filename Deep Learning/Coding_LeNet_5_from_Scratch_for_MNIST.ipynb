{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzNIOw9IqeRI"
      },
      "source": [
        "# 1. Implementing LeNet-5 - Reading the paper\n",
        "\n",
        "We're going to implement one of the first successful architectures in Convolutional Neural Networks, applied to handwritten digit recognition.\n",
        "\n",
        "This model is called LeNet-5 and was introduced in the paper [\"Gradient-Based Learning Applied To Document Recognition\"](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) in the year 1998 by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
        "\n",
        "Try to read and understand the cited paper. You don't need to read it all (it is ~46 pages long) but at least try reading up to page 7, where the CNN architecture is described.\n",
        "\n",
        "You should be able to use Keras library to build the model defining all the layers that composed it. Note that \"Subsampling\" layers are approximately equivalent to MaxPooling layers in this context. Also, the \"Gaussian\" or \"RBF\" final layer can be replaced by a Softmax in your implementation.\n",
        "\n",
        "Use the [MNIST dataset](https://keras.io/api/datasets/mnist/) also provided by Keras to train and validate your model.\n",
        "\n",
        "Try to answer:\n",
        "1. How many convolutional layers the model has?\n",
        "2. Which kind of data pre-processing is the paper using?\n",
        "3. Which activation functions are used?\n",
        "4. What is the reported model accuracy in the paper?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-gox-8IbVrQ"
      },
      "source": [
        "## 2. LeNet-5 implementation\n",
        "\n",
        "To help you in the implementation, you can refer to this diagram of the model. Note that the input images for the MNIST dataset are of size `28x28` instead of the `32x32` indicated in the original paper.\n",
        "\n",
        "You can start by defining a [Sequential model](https://keras.io/guides/sequential_model/) model in terms of [Keras layers](https://keras.io/api/layers/). Note that you'll mostly use [Convolutional](https://keras.io/api/layers/convolution_layers/convolution2d/), [MaxPooling](https://keras.io/api/layers/pooling_layers/max_pooling2d/) and [Dense](https://keras.io/api/layers/core_layers/dense/) layers. We also recommend using an [InputLayer layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer) to specify the size of the input images.\n",
        "\n",
        "\n",
        "\n",
        "![lenet.png](https://raw.githubusercontent.com/MostafaGazar/mobile-ml/master/files/lenet.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nQGOMbNqdvA"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "# YOUR IMPLEMENTATION HERE (START)\n",
        "\n",
        "# YOUR IMPLEMENTATION HERE (END)\n",
        "])\n",
        "\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LX5C448DTD6"
      },
      "source": [
        "# 3. Data loading\n",
        "\n",
        "You can load the data for the MNIST dataset with a single keras function: `tf.keras.datasets.mnist.load_data`. Note that Keras Conv2D layers expect inputs with shape `NxHxWxC` where `N` is the number of samples, `HxW` is the size of the images`C` is the number of channels. However, by default the MNIST dataset is loaded as a `NxHxW` matrix: `60000x28x28` for the training images and `10000x28x28` for the test images. Since the images are grayscale, there is only one channel and therefore the corresponding singleton dimension was ommited. **You** will have to [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) the  matrices so that they are of size `Nx28x28x1` to fit the size expected by Keras or you'll get an error when trying to train the model.\n",
        "\n",
        "You should also normalize the pixel values so that the mean of all pixels of all training images is 0 and the standard deviation is 1. Note that in this case since we only have a single channel, this is simpler, but for multichannel input images you will have to normalize each channel separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miAy_GdYDSsd"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# YOUR IMPLEMENTATION HERE (START)\n",
        "\n",
        "\n",
        "# YOUR IMPLEMENTATION HERE (END)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSE5EWWME9aq"
      },
      "source": [
        "# 4. Training the model\n",
        "\n",
        "CNNs are trained just the same as any NN model; you'll need to `compile` the model with an optimizer, a loss function, and some metrics to monitor the performance of the model.\n",
        "\n",
        "Since this is a classification problem, you'll use a `categorical_crossentropy` loss, but check the encoding of the outputs to decide if it is `sparse` (labels) or dense (one-hot encoding).\n",
        "\n",
        "\n",
        "Afterwards, you can just call the `fit` function as usual. Training for 30 epochs with a batch size of 128 should suffice to obtain a reasonably good accuracy on the test set (~97%). Make sure you include the test data as `validation_data` in the fit function so that we can compare the training and testing accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPZP_Z_FE8p8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# (1) Compile and (2) train (fit) the model with SGD, using cross entropy\n",
        "# YOUR IMPLEMENTATION HERE (START)\n",
        "\n",
        "\n",
        "# YOUR IMPLEMENTATION HERE (END)\n",
        "\n",
        "# use the `history` object returned by `fit` to plot the evolution of the accuracy and loss during training.\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "  plt.figure()\n",
        "  plt.plot(history.history['accuracy'], label='Train accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label = 'Test accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([0, 1])\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(history.history['loss'], label='Train loss')\n",
        "  plt.plot(history.history['val_loss'], label = 'Test loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(loc='lower right')\n",
        "\n",
        "plot_history(history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}